{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect category classification using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchtext.data import Field\n",
    "from torchtext.data import Iterator , BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth' , -1)\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "from torchtext.data import Field \n",
    "from torchtext.data import Iterator , BucketIterator\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data (2206, 5)\n",
      "Test data (649, 5)\n"
     ]
    }
   ],
   "source": [
    "train_grp_df = pd.read_csv('../data/resturant_train_stratified_grouped.csv')\n",
    "test_grp_df  = pd.read_csv('../data/resturant_test_stratified_grouped.csv')\n",
    "print('Train data', train_grp_df.shape) \n",
    "print('Test data',test_grp_df.shape)\n",
    "\n",
    "import ast \n",
    "train_grp_df['aspects'] = train_grp_df['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "train_grp_df['polarities'] = train_grp_df['polarities'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "test_grp_df['aspects'] = test_grp_df['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "test_grp_df['polarities'] = test_grp_df['polarities'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing column format to input it to TorchText data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = ['ambience' , 'anecdotes/miscellaneous' , 'food' , 'service' , 'price']\n",
    "for col in aspects:\n",
    "    train_grp_df[col] = train_grp_df['aspects'].apply(lambda x: 1 if col in x else 0)\n",
    "    test_grp_df[col] = test_grp_df['aspects'].apply(lambda x: 1 if col in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grp_df.drop(columns=['aspects' , 'polarities' , 'length'] , inplace=True)\n",
    "test_grp_df.drop(columns=['aspects' , 'polarities' , 'length'] , inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grp_df[['text'] + aspects].to_csv('../data/resturant_train_stratified_grouped2.csv' , index = False)\n",
    "test_grp_df[['text'] + aspects].to_csv('../data/resturant_test_stratified_grouped2.csv' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = 'spacy'\n",
    "TEXT = Field(sequential=True , tokenize=tokenize , lower=True , include_lengths=True)\n",
    "LABEL = Field(sequential=False , use_vocab=False, dtype=torch.float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    " \n",
    "train_datafields = [ ('text' , TEXT) , ('ambience' , LABEL) , ('anecdotes/miscellaneous' , LABEL) , \n",
    "                   ('food' , LABEL) , ('price' , LABEL) , ('service' , LABEL) ]\n",
    "train_data, valid_data  = TabularDataset.splits(\n",
    "            path='../data',  \n",
    "            train='resturant_train_stratified_grouped2.csv',   \n",
    "            validation='resturant_test_stratified_grouped2.csv',      \n",
    "            format='csv',\n",
    "            skip_header=True,  fields=train_datafields )\n",
    "\n",
    "\n",
    "test_datafields = [('text' , TEXT) , ('ambience' , LABEL) , ('anecdotes/miscellaneous' , LABEL) , \n",
    "                   ('food' , LABEL) , ('price' , LABEL) , ('service' , LABEL) ]\n",
    "\n",
    "test_data = TabularDataset(\n",
    "            path = '../data/resturant_test_stratified_grouped2.csv' ,\n",
    "            format='csv' , skip_header=True , \n",
    "            fields=test_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'ambience', 'anecdotes/miscellaneous', 'food', 'price', 'service'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i', 'recommend', 'to', 'anyone', 'who', 'wants', 'to', 'dress',\n",
       "       'up', 'and', 'impress', 'the', 'lady', '.'], dtype='<U9')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_data[1].__dict__['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use Glove word embeddings and keep max vocab size or around 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 4000\n",
    "BATCH_SIZE = 64\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors =  \"glove.42B.300d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 2206\n",
      "Number of validation examples: 649\n",
      "Number of test examples: 649\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of test examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    repeat=False\n",
    ")\n",
    "\n",
    "test_iter = Iterator(test_data, batch_size=1, device=device, sort=False, sort_within_batch=False, repeat=False ,\n",
    "                 train=False , shuffle=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, iterator, x_var, y_vars):\n",
    "        self.iterator, self.x_var, self.y_vars = iterator, x_var, y_vars\n",
    "  \n",
    "    def __iter__(self):\n",
    "        for batch in self.iterator:\n",
    "            x = getattr(batch, self.x_var)\n",
    "            if self.y_vars is not None:\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "            yield (x, y)\n",
    "    def __len__(self):\n",
    "        return len(self.iterator)\n",
    "\n",
    "train_loader = BatchWrapper(train_iter, \"text\", [ 'ambience', 'anecdotes/miscellaneous', 'food', 'price', 'service'])\n",
    "valid_loader = BatchWrapper(val_iter, \"text\", [ 'ambience', 'anecdotes/miscellaneous', 'food', 'price', 'service'])\n",
    "test_loader = BatchWrapper(test_iter, \"text\"   ,[ 'ambience', 'anecdotes/miscellaneous', 'food', 'price', 'service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)        \n",
    "        self.rnn = nn.LSTM(embedding_dim,  hidden_dim, num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2 , output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence( embedded, text_lengths)\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        #output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)       \n",
    "        hidden =(torch.cat((hidden[-2,:,:],  hidden[-1,:,:]), dim = 1))  \n",
    "   \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 5\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,441,605 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters() , lr = 1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0072,  0.0380,  0.0675,  ...,  0.0396,  0.0018,  0.0760],\n",
       "        [-0.1117, -0.4966,  0.1631,  ..., -1.4447,  0.8402, -0.8668],\n",
       "        [ 0.1088,  0.0022,  0.2221,  ..., -0.2970,  0.1594, -0.1490],\n",
       "        ...,\n",
       "        [-0.2680, -0.9008,  0.1308,  ...,  0.2303, -0.5960,  0.3290],\n",
       "        [-0.5858, -0.3707, -0.1245,  ..., -0.0055, -0.8436,  0.0873],\n",
       "        [ 0.3440,  0.8542, -0.5785,  ...,  0.1280,  0.0526,  0.3247]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_func = loss_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    acc = torch.sum(rounded_preds == y).float() \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    model.train()\n",
    "    ct = 0\n",
    "    for x, y in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths =x\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, y)\n",
    "        acc = binary_accuracy(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        ct = ct + text.shape[1]   \n",
    "    return epoch_loss / len(iterator), epoch_acc / (ct*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    ct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x ,y  in iterator:\n",
    "            text, text_lengths = x\n",
    "            predictions = model(text, text_lengths)#.squeeze(1)\n",
    "            loss = criterion(predictions,y)\n",
    "            acc = binary_accuracy(predictions, y)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            ct = ct + text.shape[1]\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / (ct*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss!! 0.3941935653036291\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.418 | Train Acc: 81.50%\n",
      "\t Val. Loss: 0.394 |  Val. Acc: 84.68%\n",
      "Best validation loss!! 0.27863858301531186\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.211 | Train Acc: 91.77%\n",
      "\t Val. Loss: 0.279 |  Val. Acc: 90.02%\n",
      "Best validation loss!! 0.2771649191325361\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.116 | Train Acc: 95.61%\n",
      "\t Val. Loss: 0.277 |  Val. Acc: 90.42%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.061 | Train Acc: 97.77%\n",
      "\t Val. Loss: 0.344 |  Val. Acc: 90.32%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.86%\n",
      "\t Val. Loss: 0.364 |  Val. Acc: 90.26%\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "best_epoch = 0\n",
    "        \n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss , train_acc = train_model(model, train_loader, optimizer, loss_func)\n",
    "    valid_loss , valid_acc = validate_model(model, valid_loader, loss_func)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_epoch = epoch\n",
    "        print(f'Best validation loss!! {best_valid_loss}')\n",
    "        torch.save(model.state_dict(), 'aspect_category.pt')\n",
    "    #print(f'Epoch: {epoch}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "text_data = []\n",
    "true_label = []\n",
    "\n",
    "epoch_loss = 0\n",
    "epoch_acc = 0 \n",
    "ct = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x ,y  in test_loader:\n",
    "        text, text_lengths = x\n",
    "        text_data.append(text.data.cpu().numpy())\n",
    "        predictions = model(text, text_lengths)#.squeeze(1)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        test_preds.append(preds)\n",
    "        true_label.append(y.data.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merging prediction value with original test data and observe the metrics on overall level\n",
    "\"\"\"\n",
    "test_pred = pd.DataFrame(np.vstack(test_preds) ,index=test_grp_df.index , columns= [a+'_pred' for a in aspects])\n",
    "test_aspect_pred = pd.merge(test_grp_df, test_pred , left_index=True ,right_index = True)\n",
    "\n",
    "from sklearn.metrics import f1_score , confusion_matrix , accuracy_score , precision_score , recall_score , roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.805298415421371\n",
      "Accuracy score 0.9026194144838212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score\",f1_score(test_aspect_pred[aspects].as_matrix() , test_aspect_pred[test_aspect_pred.columns[-5:]].as_matrix() , average='macro' ))\n",
    "print(\"Accuracy score\" , np.mean(test_aspect_pred[aspects].as_matrix() == test_aspect_pred[test_aspect_pred.columns[-5:]].as_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for different aspect category \n",
      "ambience :  0.9090909090909091\n",
      "anecdotes/miscellaneous :  0.8228043143297381\n",
      "food :  0.8859784283513097\n",
      "service :  0.9322033898305084\n",
      "price :  0.963020030816641\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for different aspect category ')\n",
    "for col in aspects:\n",
    "    print(col , ': ', test_aspect_pred[test_aspect_pred[col]==test_aspect_pred[col+'_pred']].shape[0]/test_aspect_pred.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for the accuracy of conflicting statements (sentiment changing within a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.7492912295251918\n",
      "Accuracy score 0.8368932038834952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test2_conflict = test_aspect_pred[test_aspect_pred['ind']==1]\n",
    "print(\"F1 score\",f1_score(test2_conflict[aspects].as_matrix() , test2_conflict[test2_conflict.columns[-5:]].as_matrix() , average='macro' ))\n",
    "print(\"Accuracy score\" , np.mean(test2_conflict[aspects].as_matrix() == test2_conflict[test2_conflict.columns[-5:]].as_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for different aspect category \n",
      "ambience :  0.8349514563106796\n",
      "anecdotes/miscellaneous :  0.7378640776699029\n",
      "food :  0.8737864077669902\n",
      "service :  0.8349514563106796\n",
      "price :  0.9029126213592233\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for different aspect category ')\n",
    "for col in aspects:\n",
    "    print(col , ': ', test2_conflict[test2_conflict[col]==test2_conflict[col+'_pred']].shape[0]/test2_conflict.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that accuracy and F1 score has improved significantly from TfIdf vector based classifier. We will use prediction of this model to find aspect categories in statement and then feed that to sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_pred(x):\n",
    "    res = []\n",
    "    for col in aspects:\n",
    "        if x[col+'_pred']==1:\n",
    "            res.append(col)\n",
    "    return res\n",
    "\n",
    "test_aspect_pred['aspects_pred'] = test_aspect_pred.apply(lambda x: aspect_pred(x) ,axis =1)\n",
    "test_aspect_pred.drop(columns=['anecdotes/miscellaneous_pred', 'food_pred', 'service_pred', 'price_pred', 'ambience_pred'] , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_pred(x):\n",
    "    res = []\n",
    "    for col in aspects:\n",
    "        if x[col]==1:\n",
    "            res.append(col)\n",
    "    return res\n",
    "\n",
    "test_aspect_pred['aspects'] = test_aspect_pred.apply(lambda x: aspect_pred(x) ,axis =1)\n",
    "test_aspect_pred.drop(columns=aspects , inplace=True)\n",
    "test_aspect_pred.to_csv('../output/BiLSTM_aspect_prediction_test_data.csv' , index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using aspect prediction on test data , we have segmented some sentence using constituency parsing. Please go through: 2- Sentence segmentation using constituency parsing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    598\n",
       "2    45 \n",
       "3    5  \n",
       "4    1  \n",
       "Name: length_splits, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "test_segments = pd.read_csv('../output/test_data_sentence_segmentation.csv')\n",
    "for col in ['aspects_pred' , 'aspects' , 'splits']:\n",
    "    test_segments[col]  = test_segments[col].apply(lambda x: ast.literal_eval(x))\n",
    "    \n",
    "test_segments['length_splits'] = test_segments['splits'].apply(lambda x: len(x))\n",
    "test_segments.length_splits.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews with only one aspect or no segment resulting from constituency parsing 598\n",
      "Reviews with more than one sentence segment resulting from constituency parsing  51\n"
     ]
    }
   ],
   "source": [
    "single_aspect_df = test_segments[(test_segments['length_splits']==1)] # & (test_aspect_pred['ct_aspects']<=1)\n",
    "multi_aspect_df = test_segments[test_segments['length_splits']>1]\n",
    "multi_aspect_df.drop(columns=['aspects' , 'ct_aspects', 'length_splits' ] , inplace=True)\n",
    "print('Reviews with only one aspect or no segment resulting from constituency parsing' , single_aspect_df.shape[0] )\n",
    "print('Reviews with more than one sentence segment resulting from constituency parsing ' , multi_aspect_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reviews with different segments mapping to multiple aspect categories, we will use pretrained Bi-LSTM model to predict most prominent aspect category in sentence split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_aspects_splits= []\n",
    "for index, row in multi_aspect_df.iterrows():                   \n",
    "    total_aspects = row['aspects_pred'].copy()\n",
    "    total_sent = row['splits'].copy()    \n",
    "    for sent in total_sent:\n",
    "        multi_aspects_splits.append({  'id': index , 'sp_text': ' '.join(sent) , 'aspects_pred': total_aspects })\n",
    "\n",
    "df_multi_aspects_splits = pd.DataFrame(multi_aspects_splits)\n",
    "df_multi_aspects_splits[['sp_text','id']].to_csv('../output/test_data_split_ungrped.csv' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datafields = [('sp_text' , TEXT) ]\n",
    "test_data = TabularDataset( path = '../output/test_data_split_ungrped.csv' ,  format='csv' , skip_header=True ,  fields=test_datafields)\n",
    "test_iter2 = Iterator(test_data, batch_size=1, device=device, sort=False, sort_within_batch=False, repeat=False , train=False , shuffle=False )\n",
    "test_loader = BatchWrapper(test_iter2, \"sp_text\" , None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x ,y  in test_loader:\n",
    "        text, text_lengths = x\n",
    "        text_data.append(text.data.cpu().numpy())\n",
    "        predictions = model(text, text_lengths)#.squeeze(1)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        test_preds.append(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpreddf = pd.DataFrame(np.vstack(test_preds) ,index=df_multi_aspects_splits.index , columns= [a+'_pred' for a in aspects])\n",
    "df_multi_aspects_splits = pd.merge(df_multi_aspects_splits, testpreddf , left_index=True ,right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_pred(x):\n",
    "    res = []\n",
    "    for col in aspects: \n",
    "        if x[col+'_pred']==1:  res.append(col)\n",
    "    return res\n",
    "\n",
    "df_multi_aspects_splits['phrase_aspect'] = df_multi_aspects_splits.apply(lambda x: aspect_pred(x) ,axis =1)\n",
    "df_multi_aspects_splits.drop(columns=['anecdotes/miscellaneous_pred', 'food_pred', 'service_pred', 'price_pred', 'ambience_pred'] , inplace=True)\n",
    "df_multi_aspects_splits['remaining_aspect'] = df_multi_aspects_splits.apply(lambda x: list(set(x['aspects_pred']) - set(x['phrase_aspect']))  , axis =1)\n",
    "df_multi_aspects_splits['remaining_aspect'] = df_multi_aspects_splits['remaining_aspect'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi_aspects_splits['shift_id'] = df_multi_aspects_splits['id'].shift(1)\n",
    "df_multi_aspects_splits.shift_id.fillna(0 , inplace = True)\n",
    "df_multi_aspects_splits['shift_id'] =  df_multi_aspects_splits.apply(lambda x: 1 if x['id']!= x['shift_id']  else 0 , axis = 1 )\n",
    "df_multi_aspects_splits['remaining_aspect']  = df_multi_aspects_splits.apply(lambda x: [] if x['shift_id']==1 else x['remaining_aspect'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi_aspects_splits2= []\n",
    "for index, row in df_multi_aspects_splits.iterrows():                   \n",
    "    total_aspects = row['phrase_aspect'].copy() \n",
    "    if len(total_aspects) ==1:\n",
    "        df_multi_aspects_splits2.append({'id': row['id']  , 'sp_text':row['sp_text'] ,'fn_aspect': total_aspects[0] })\n",
    "    else :\n",
    "        if len(row['remaining_aspect']) ==1:\n",
    "            df_multi_aspects_splits2.append({'id': row['id']  , 'sp_text':row['sp_text'] ,'fn_aspect': row['remaining_aspect'][0] })\n",
    "        elif len(row['remaining_aspect']) ==0:\n",
    "            for asp in row['phrase_aspect']:\n",
    "                df_multi_aspects_splits2.append({'id': row['id'], 'sp_text':row['sp_text'],'fn_aspect':asp})\n",
    "        else:\n",
    "            for asp in row['remaining_aspect']:\n",
    "                df_multi_aspects_splits2.append({'id': row['id'], 'sp_text':row['sp_text'],'fn_aspect':asp})\n",
    "                \n",
    "\n",
    "df1_aspect_lvl= pd.DataFrame(df_multi_aspects_splits2)\n",
    "df1_aspect_lvl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ind</th>\n",
       "      <th>aspects_pred</th>\n",
       "      <th>splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We were seated outside and the waiter spilled red wine and hot tea on myself and my date.</td>\n",
       "      <td>0</td>\n",
       "      <td>[food, service]</td>\n",
       "      <td>[[We, were, seated, outside], [the, waiter, spilled, red, wine, and, hot, tea, on, myself, and, my, date]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The crust is thin, the ingredients are fresh and the staff is friendly.</td>\n",
       "      <td>0</td>\n",
       "      <td>[food, service]</td>\n",
       "      <td>[[The, crust, is, thin], [the, ingredients, are, fresh], [the, staff, is, friendly]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        text  \\\n",
       "3  We were seated outside and the waiter spilled red wine and hot tea on myself and my date.   \n",
       "4  The crust is thin, the ingredients are fresh and the staff is friendly.                     \n",
       "\n",
       "   ind     aspects_pred  \\\n",
       "3  0    [food, service]   \n",
       "4  0    [food, service]   \n",
       "\n",
       "                                                                                                       splits  \n",
       "3  [[We, were, seated, outside], [the, waiter, spilled, red, wine, and, hot, tea, on, myself, and, my, date]]  \n",
       "4  [[The, crust, is, thin], [the, ingredients, are, fresh], [the, staff, is, friendly]]                        "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_aspect_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_aspect= []\n",
    "for index, row in single_aspect_df.iterrows():\n",
    "    try:\n",
    "        aspect = row['aspects_pred'][0]\n",
    "    except:\n",
    "        aspect =''\n",
    "    single_aspect.append({'id': index, 'sp_text':row['text'],'fn_aspect':aspect})               \n",
    "\n",
    "df2_aspect_lvl= pd.DataFrame(single_aspect)\n",
    "df2_aspect_lvl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_lvl_dataframe = pd.concat([df1_aspect_lvl , df2_aspect_lvl] , axis = 0)\n",
    "aspect_lvl_dataframe.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_lvl_dataframe.sort_values(by = ['id']  , inplace=True)\n",
    "aspect_lvl_dataframe.reset_index(inplace= True , drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_lvl_dataframe.to_csv('../output/aspect_level_test_data.csv' , index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
